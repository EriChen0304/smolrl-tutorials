{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Q learning & Deep Q Network (DQN)\n",
    "\n",
    "References:\n",
    "\n",
    "- [rezaborhani's blog: Q-Learning](https://rezaborhani.github.io/mlr/blog_posts/Reinforcement_Learning/Q_learning.html)\n",
    "- [Q-Learning Guide: Begin with Reinforcement Learning Basics](https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning)\n",
    "- [Wikipedia: Q-learning](https://en.wikipedia.org/wiki/Q-learning)\n",
    "- [Mathematical Foundations of Reinforcement Learning](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)\n",
    "  - [Lu, Yukuan: Q Learning](https://lyk-love.cn/2024/06/22/q-learning/)\n",
    "\n",
    "Resources:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "\n",
    "Ref:\n",
    "\n",
    "- Christopher Watkins and Peter Dayan, Q-learning, Machine learning, 8.3-4, 1992, 279-292. [pdf](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf), [errata](https://www.gatsby.ucl.ac.uk/~dayan/papers/cjcherr.pdf)\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. 初始化所有状态-动作对 (state-action pairs) 的 Q 值 $Q(s, a)$ (根据实现的策略，可以使用随机值或者服从某些分布)\n",
    "2. 进行迭代算法 (For life or until learning is stopped)\n",
    "   1. 根据当前 Q 值估计 (Q-value estimates) $Q(s, \\cdot)$ 在当前世界状态 ($s$) 中选择一个动作 ($a$)。\n",
    "   2. 执行动作 ($a$) 并观察结果状态 ($s’$) 和奖励 ($r)。\n",
    "   3. 对真实 Q 值进行更新 $Q' (s, a) := Q(s,a) + \\alpha [r + \\gamma\\max_{\\substack a'} Q (s', a') - Q(s, a)]$\n",
    "\n",
    "<!-- ![Q_Learning_Process](./02.assets/Q_Learning_Process_134331efc1.png)\n",
    "\n",
    "source: [DataCamp - An Introduction to Q-Learning: A Tutorial For Beginners](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Table\n",
    "\n",
    "Q Table <-> Pair of (State space, Action) $(s_i, a_j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update process\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{t+1}(s_t, a_t)\n",
    "&\\gets Q_t(s_t, a_t) + \\alpha_t ( r_{t+1} + \\gamma \\cdot \\max_{a \\in \\mathcal A_{s_{t+1}}} Q_t( s_{t+1} , a) - Q_t (s_{t+1}, a_t)) \\\\\n",
    "&= (1 - \\alpha)Q(s_t, a_t) + \\alpha r_{t+1} + \\alpha \\gamma \\max_{a \\in \\mathcal A_{s_{t+1}}} Q_t( s_{t+1} , a)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心概念\n",
    "\n",
    "- Action Value function $Q(s, a)$ (价值函数)\n",
    "  - 当前状态 $s$ 下执行行动 $a$ 的 $Q$ 值。\n",
    "- $s'$ <-> $s_{t+1}$\n",
    "  - next state, **predicated**\n",
    "  - 是执行行动 $a$ 后的新状态。\n",
    "- Learning rate $\\alpha$ (学习率)\n",
    "  - A factor determining how much new information overrides old information.\n",
    "  - A higher learning rate means the agent learns faster, updating its Q-values more significantly with new rewards and experiences.\n",
    "  - 学习率，决定了**新获得**的信息对现有 Q 值的影响程度。\n",
    "- Discounting rate $\\gamma$ (衰减因子)\n",
    "  - This factor discounts the value of future rewards compared to immediate rewards.\n",
    "  - A higher discount factor means that future rewards are more valuable, encouraging long-term beneficial actions over short-term gains.\n",
    "  - 折扣因子，用于平衡即时奖励和未来奖励的重要性。\n",
    "- $R_{t+1}$ <-> $R(s_t \\to s_{t+1}, a_t)$\n",
    "  - 是执行行动 $a_t$ 后，从 $s_t$ 转换到 $s_{t+1}$ 所获得的即时奖励。\n",
    "- Epsilon-Greedy Strategy $\\varepsilon$ (贪婪策略因子)\n",
    "\n",
    "$Q_{t+1}(s_t, a_t)$ is the sum of three parts:\n",
    "\n",
    "1. $(1 - \\alpha)Q(s_t, a_t)$: the current value (weighted by one minus the learning rate)\n",
    "2. $\\alpha r_{t+1}$: The reward $r_{t+1}$ to obtain if action $a_t$ is taken when in State $s_t$ (weighted by learning rate)\n",
    "3. $\\max_{a \\in \\mathcal A_{s_{t+1}}} Q_t( s_{t+1} , a)$: the maximum reward that can be obtained from state $s_{t+1}$ (weighted by learning rate and discount factor)\n",
    "\n",
    "What we learn actually?\n",
    "\n",
    "**Policy $\\pi$**\n",
    "\n",
    "- The policy determines what action to take in each state and can be derived from the Q-values.\n",
    "- Typically, **the policy chooses the action with the highest Q-value in each state (exploitation)**, though sometimes a less optimal action is chosen for exploration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image illustration\n",
    "\n",
    "![Equation Visuals from Thomas Simonini](./02.assets/image_d200c39908.png)\n",
    "\n",
    "source: [DataCamp - An Introduction to Q-Learning: A Tutorial For Beginners](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial): Author Thomas Simonini\n",
    "\n",
    "![q-sum-of-3-parts](./02.assets/q-sum-of-3-parts.png)\n",
    "\n",
    "source: [Q-learning - Wikipedia](https://en.wikipedia.org/wiki/Q-learning)\n",
    "\n",
    "![exp-replay](./02.assets/exp-replay.png)\n",
    "\n",
    "source: [\\[RL\\] Q learning 與 Deep Q Network(DQN)](https://hackmd.io/@YungHuiHsu/BJgnMHbUH6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-greedy 策略的工作原理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation (贝尔曼方程)\n",
    "\n",
    "The Bellman equation is a fundamental concept in dynamic programming and reinforcement learning. Essentially, the Bellman equation breaks down the decision-making problem into smaller, manageable subproblems and then combines their solutions to determine the optimal policy.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_t|s_t=s] = \\sum_{a}\\pi(a|s)\\sum_{s'}P(s'|s,a)\\left[ R(s,a,s') + \\gamma V(s') \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录 (Appendix)\n",
    "\n",
    "绘制 Q-table 或奖励曲线\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
