{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Q learning & Deep Q Network (DQN)\n",
    "\n",
    "References:\n",
    "\n",
    "- [rezaborhani's blog: Q-Learning](https://rezaborhani.github.io/mlr/blog_posts/Reinforcement_Learning/Q_learning.html)\n",
    "- [Q-Learning Guide: Begin with Reinforcement Learning Basics](https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning)\n",
    "- [Wikipedia: Q-learning](https://en.wikipedia.org/wiki/Q-learning)\n",
    "\n",
    "Resources:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "\n",
    "Ref:\n",
    "\n",
    "- Christopher Watkins and Peter Dayan. Q-learning. Machine learning 8.3-4 (1992) : 279-292.\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- Learning rate $\\alpha$ (学习率)\n",
    "  - A factor determining how much new information overrides old information.\n",
    "  - A higher learning rate means the agent learns faster, updating its Q-values more significantly with new rewards and experiences.\n",
    "- Discounting rate $\\gamma$ (衰减因子)\n",
    "  - This factor discounts the value of future rewards compared to immediate rewards.\n",
    "  - A higher discount factor means that future rewards are more valuable, encouraging long-term beneficial actions over short-term gains.\n",
    "- Epsilon-Greedy Strategy $\\epsilon$ (贪婪策略因子)\n",
    "- Action Value function $Q$ (价值函数)\n",
    "- Policy $\\pi$\n",
    "  - The policy determines what action to take in each state and can be derived from the Q-values.\n",
    "  - Typically, the policy chooses the action with the highest Q-value in each state (exploitation), though sometimes a less optimal action is chosen for exploration purposes.\n",
    "- Q Table <-> Pair of (State space, Action) $(s_i, a_j)$\n",
    "\n",
    "### 算法流程\n",
    "\n",
    "1. 初始化所有状态-动作对 (state-action pairs) 的 Q 值 $Q(s, a)$ (根据实现的策略，可以使用随机值或者服从某些分布)\n",
    "2. 进行迭代算法 (For life or until learning is stopped)\n",
    "   1. 根据当前 Q 值估计 (Q-value estimates) $Q(s, \\cdot)$ 在当前世界状态 ($s$) 中选择一个动作 ($a$)。\n",
    "   2. 执行动作 ($a$) 并观察结果状态 ($s’$) 和奖励 ($r)。\n",
    "   3. 对真实 Q 值进行更新 $Q (s, a) := Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q (s', a') - Q(s, a)]$\n",
    "\n",
    "![Q_Learning_Process](https://images.datacamp.com/image/upload/v1666973295/Q_Learning_Process_134331efc1.png)\n",
    "\n",
    "source: [DataCamp - An Introduction to Q-Learning: A Tutorial For Beginners](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q'(s_t, a_t)\n",
    "&\\gets Q(s_t, a_t) + \\alpha ( r_t + \\gamma \\cdot \\mathrm{max}_{a} Q( s_{t+1} , a) - Q (s_t, a_t)) \\\\\n",
    "&= (1 - \\alpha)Q(s_t, a_t) + \\alpha r_{t+1} + \\alpha\\gamma\\max_{a}Q(s_{t+1}, a)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$Q'(s_t, a_t)$ is the sum of three parts:\n",
    "\n",
    "1. $(1 - \\alpha)Q(s_t, a_t)$: the current value (weighted by one minus the learning rate)\n",
    "2. $\\alpha r_{t+1}$: The reward $r_{t+1}$ to obtain if action $a_t$ is taken when in State $s_t$ (weighted by learning rate)\n",
    "3. $\\alpha\\gamma\\max_{a}Q(s_{t+1}, a)$: the maximum reward that can be obtained from state $s_{t+1}$ (weighted by learning rate and discount factor)\n",
    "\n",
    "![Equation Visuals from Thomas Simonini](https://media.datacamp.com/legacy/v1725960040/image_d200c39908.png)\n",
    "\n",
    "source: [DataCamp - An Introduction to Q-Learning: A Tutorial For Beginners](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial): Author Thomas Simonini\n",
    "\n",
    "![q-sum-of-3-parts](./02.assets/q-sum-of-3-parts.png)\n",
    "\n",
    "source: [Q-learning - Wikipedia](https://en.wikipedia.org/wiki/Q-learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation (贝尔曼方程)\n",
    "\n",
    "The Bellman equation is a fundamental concept in dynamic programming and reinforcement learning. Essentially, the Bellman equation breaks down the decision-making problem into smaller, manageable subproblems and then combines their solutions to determine the optimal policy.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_t|s_t=s] = \\sum_{a}\\pi(a|s)\\sum_{s'}P(s'|s,a)\\left[ R(s,a,s') + \\gamma V(s') \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deep?\n",
    "\n",
    "![]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录 (Appendix)\n",
    "\n",
    "绘制 Q-table 或奖励曲线\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
