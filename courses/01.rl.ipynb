{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Introduction\n",
    "\n",
    "References:\n",
    "\n",
    "- [ehennis/ReinforcementLearning](https://github.com/ehennis/ReinforcementLearning): Repository for my CS6460 Education Technology Project\n",
    "- [Hugging Face - Deep RL Course - Introduction to Deep Reinforcement Learning](https://huggingface.co/learn/deep-rl-course/unit1/rl-framework)\n",
    "\n",
    "Resources:\n",
    "\n",
    "- Article [naklecha's site - a reinforcement learning guide](https://naklecha.notion.site/a-reinforcement-learning-guide)\n",
    "- Textbook [Reinforcement Learning: An Introduction - 2nd](http://www.incompleteideas.net/book/the-book.html)\n",
    "- MOOC\n",
    "  - [Coursera: Reinforcement Learning Specialization](https://www.coursera.org/specializations/reinforcement-learning)\n",
    "  - [HuggingFace: huggingface/deep-rl-class](https://github.com/huggingface/deep-rl-class): This repo contains the Hugging Face Deep Reinforcement Learning Course.\n",
    "- [OpenAI Spinning Up - Welcome to Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/index.html)\n",
    "\n",
    "![mice-maze](https://maze.conductscience.com/wp-content/uploads/2019/09/Theseus-exploring-a-maze-in-history-of-mazes1.jpg)\n",
    "\n",
    "source: [History of Mazes](https://maze.conductscience.com/historical-mazes/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Basics\n",
    "\n",
    "- Decision Making\n",
    "- Markov Decision Process (MDP)\n",
    "  - Probability Model -> Probabilty Graph Model (PGM)\n",
    "- Value Iteration\n",
    "- Policy Iteration\n",
    "- Deterministic and Stochastic Movements\n",
    "\n",
    "## 理解强化学习的核心概念\n",
    "\n",
    "- Agent (智能体)\n",
    "- Environment (环境)\n",
    "- Reward (奖励)\n",
    "- More\n",
    "  - state (状态)\n",
    "  - observations (观测)\n",
    "    - full observability\n",
    "    - partial observability\n",
    "  - action spaces (动作空间)\n",
    "  - policy (策略) => next action 下一步的动作\n",
    "  - value functions (值函数) => 基于奖励 (reward) 构造\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg)\n",
    "\n",
    "source: wikipedia\n",
    "\n",
    "![RL_process](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg)\n",
    "\n",
    "source: [HuggingFace Deep RL Course](https://huggingface.co/learn/deep-rl-course/unit1/rl-framework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "- $\\cal{S}$ or $S$\n",
    "  - A set of environment and agent states (环境状态的集合)\n",
    "  - $\\cal S = \\{ s_1, s_2, s_3, ..., s_i \\}$\n",
    "- $\\cal A$ or $A$\n",
    "  - A set of actions (the action space) (动作的集合)\n",
    "  - $\\cal A = \\{ a_1, a_2, ..., a_i \\}$\n",
    "- $P_a(s, s') = \\Pr (S_{t+1} = s' | S_t = s, A_t = a)$\n",
    "  - the transition probability (at time $t$) from state $s$ to state $s'$ under action $a$.\n",
    "  - 在状态之间转换的规则（转移概率矩阵）\n",
    "- $R_a(s, s')$\n",
    "  - the immediate reward after transition from $s$ to $s'$ under action $a$.\n",
    "  - 规定转换后“即时奖励”的规则（奖励函数）\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg)\n",
    "\n",
    "source: [HuggingFace Deep RL Course](https://huggingface.co/learn/deep-rl-course/unit1/rl-framework)\n",
    "\n",
    "> From Wikipedia (tunned):\n",
    ">\n",
    "> The agent's **action selection** is modeled as a map called policy\n",
    ">\n",
    "> $$\n",
    "> \\begin{array}{c}\n",
    "> \\pi : \\cal S \\times \\cal A \\to [0, 1] \\\\\n",
    "> \\pi (s, a) = \\Pr (_t = a | S_t > = s)\n",
    "> \\end{array}\n",
    "> $$\n",
    ">\n",
    "> The policy map gives the probability of taking action $a$ when in state $s$.\n",
    ">\n",
    "> The goal of a reinforcement learning agent is to learn a policy\n",
    "> that maximizes the expected cumulative reward.\n",
    "\n",
    "强化学习的主体与环境基于离散的时间步作用。在每一个时间 $t$，主体接收到一个观测 $o_t$，通常其中包含奖励 $r_t$。然后，它从允许的集合中选择一个动作 $a_t$，然后送出到环境中去。环境则变化到一个新的状态 $s_t$，然后决定了和这个变化 $s_t, a_t , s_{t+1}$ 相关联的奖励 $r_{t + 1}$。\n",
    "\n",
    "![RL_process_game.jpg](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The points\n",
    "\n",
    "### Exploration–exploitation Trade off\n",
    "\n",
    "What is [Exploration–exploitation dilemma](https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma): 两种对立策略之间的平衡行为。\n",
    "\n",
    "- exploitation (of current knowledge)\n",
    "  - 利用、执行\n",
    "  - 利用是指根据当前对系统的了解（可能是不完整的或误导性的）选择最佳选项\n",
    "- exploration (of uncharted territory)\n",
    "  - 探索、勘探\n",
    "  - 探索则是指尝试新的选项，这些选项可能会在未来带来更好的结果，但会牺牲利用机会。\n",
    "- more readings: [Article: Exploration Strategies in Deep Reinforcement Learning](https://lilianweng.github.io/posts/2020-06-07-exploration-drl/)\n",
    "\n",
    "![](./01.assets/exp_2.jpg)\n",
    "\n",
    "source: [HuggingFace Deep RL Course](https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff)\n",
    "\n",
    "### \"Policy\" in learning\n",
    "\n",
    "Agent Policy ($\\pi$) -> What's the next action and state $\\cal S \\times \\cal A$\n",
    "\n",
    "Learning policy\n",
    "\n",
    "- behavior policy (State value function)\n",
    "  - policy to sample state and reward\n",
    "  - 采样数据的策略为行为策略 (behavior policy)\n",
    "    - 先采样 (=> 预测)，再行动\n",
    "    - 先行动，再采样 (=> 观测)\n",
    "- target policy (Action value function)\n",
    "  - action selection\n",
    "  - 称用这些数据来更新的策略为目标策略 (target policy)\n",
    "    - eg.: exploitation / exploration\n",
    "\n",
    "Two types of learning policy\n",
    "\n",
    "- off-policy (异策略)\n",
    "  - example: Q-learning\n",
    "  - 下一个状态 $(s_{t+1}, a_{t+1})$ 依赖于 $(s_t, a_t, r_t, s_{t+1})$\n",
    "  - 先更新后根据 (评估) 策略选择动作\n",
    "    - 选择动作就是一个行动策略\n",
    "  - 行动策略 (what's the next action?) 和评估策略 (what would reward after next action) 不同\n",
    "- on-policy (同策略) -> optimal policy (近似最优)\n",
    "  - example: SARASA (State–action–reward–state–action)\n",
    "  - 下一个状态 $(s_{t+1}, a_{t+1})$ 依赖于 $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$ (五元组)\n",
    "  - 先做出动作后更新策略\n",
    "  - 当前优化的策略的样本是来自于这个策略 <=> 只使用了当前策略产生的样本\n",
    "  - 在一个迭代里行动策略和评估策略相同\n",
    "\n",
    "### Main approaches\n",
    "\n",
    "- Model free\n",
    "  - 人话: 模型自由的强化学习不依赖于环境的内部模型 (依赖外界的 feedback / Reward)\n",
    "  - 数语: 不需要知道状态之间的转移概率 (transition probability)\n",
    "- Model based\n",
    "  - 人话: 模型基础的强化学习则是依赖于环境的一个内部模型来做决策。它先尝试理解环境，然后基于这个理解来计划最优行动。\n",
    "  - 数语: 知道状态之间的转移概率\n",
    "  - 继续细分\n",
    "    - Policy Iteration: 选定策略 -> 更新策略\n",
    "    - Value Iteration: 选定值 -> 更新值\n",
    "\n",
    "> If you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action?\n",
    ">\n",
    "> -- from _Quora_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why reinforcement learning?\n",
    "\n",
    "强化学习的强大能力来源于两个方面\n",
    "\n",
    "1. 使用样本 (aka: 观测) 来优化 (aka: 学习) 行为\n",
    "2. 使用函数**近似**来描述复杂的环境。\n",
    "   - 物理人看到近似应该会很敏感吧 LoL\n",
    "\n",
    "它们使得强化学习可以使用在以下的复杂环境中：\n",
    "\n",
    "模型的环境已知，且解析解不存在；\n",
    "仅仅给出环境的模拟模型（模拟优化方法的问题）[6]\n",
    "从环境中获取信息的唯一办法是和它互动。前两个问题可以被考虑为规划问题，而最后一个问题可以被认为是 genuine learning 问题。使用强化学习的方法，这两种规划问题都可以被转化为机器学习问题。\n",
    "\n",
    "### Comparison of key algorithms\n",
    "\n",
    "- [Wikipedia: Comparison of key algorithms](https://en.wikipedia.org/wiki/Reinforcement_learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "\n",
    "### 迷宫导航\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning 通过时序差分更新实现价值函数的渐进收敛。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical CS Algorithms\n",
    "\n",
    "- 动态规划\n",
    "- Brute force (枚举) -> 回溯 + 剪枝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics x AI\n",
    "\n",
    "### Concepts\n",
    "\n",
    "- 状态空间（State Space）的物理对应（如粒子坐标）\n",
    "- 概率模型\n",
    "  - 马尔可夫过程\n",
    "  - 统计力学\n",
    "- 奖励函数设计原则\n",
    "  - 物理系统约束\n",
    "  - 最小能量原则\n",
    "  - 各种守恒量\n",
    "\n",
    "### Applications\n",
    "\n",
    "- 量子系统控制案例\n",
    "- 粒子加速器参数优化\n",
    "- 超材料结构设计\n",
    "\n",
    "- [Universal quantum control through deep reinforcement learning](https://www.nature.com/articles/s41534-019-0141-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
